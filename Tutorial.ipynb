{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12addeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import abc\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "\n",
    "# create abstract Dataset class called StickDataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "from utils.r3D_semantic_dataset import load_depth\n",
    "from utils.metrics import get_act_mean_std\n",
    "from utils.traverse_data import iter_dir_for_traj_pths\n",
    "\n",
    "\n",
    "class BaseStickDataset(Dataset, abc.ABC):\n",
    "    def __init__(self, traj_path, time_skip, time_offset, time_trim):\n",
    "        super().__init__()\n",
    "        self.traj_path = Path(traj_path)\n",
    "        self.time_skip = time_skip\n",
    "        self.time_offset = time_offset\n",
    "        self.time_trim = time_trim\n",
    "        self.img_pth = self.traj_path / \"images\"\n",
    "        self.depth_pth = self.traj_path / \"depths\"\n",
    "        self.conf_pth = self.traj_path / \"confs\"\n",
    "        self.labels_pth = self.traj_path / \"labels.json\"\n",
    "\n",
    "        self.labels = json.load(self.labels_pth.open(\"r\"))\n",
    "        self.img_keys = sorted(self.labels.keys())\n",
    "        # lable structure: {image_name: {'xyz' : [x,y,z], 'rpy' : [r, p, y], 'gripper': gripper}, ...}\n",
    "\n",
    "        self.labels = np.array(\n",
    "            [self.flatten_label(self.labels[k]) for k in self.img_keys]\n",
    "        )\n",
    "\n",
    "        # filter using time_skip and time_offset and time_trim. start from time_offset, skip time_skip, and remove last time_trim\n",
    "        self.labels = self.labels[: -self.time_trim][self.time_offset :: self.time_skip]\n",
    "\n",
    "        # filter keys using time_skip and time_offset and time_trim. start from time_offset, skip time_skip, and remove last time_trim\n",
    "        self.img_keys = self.img_keys[: -self.time_trim][\n",
    "            self.time_offset :: self.time_skip\n",
    "        ]\n",
    "\n",
    "    def flatten_label(self, label):\n",
    "        # flatten label\n",
    "        xyz = label[\"xyz\"]\n",
    "        rpy = label[\"rpy\"]\n",
    "        gripper = label[\"gripper\"]\n",
    "        return np.concatenate((xyz, rpy, np.array([gripper])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # not implemented\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class StickDataset(BaseStickDataset, abc.ABC):\n",
    "    def __init__(\n",
    "        self, traj_path, traj_len, time_skip, time_offset, time_trim, traj_skip\n",
    "    ):\n",
    "        super().__init__(traj_path, time_skip, time_offset, time_trim)\n",
    "        self.traj_len = traj_len\n",
    "        self.traj_skip = traj_skip\n",
    "        self.reformat_labels(self.labels)\n",
    "        self.act_metrics = None\n",
    "\n",
    "    def set_act_metrics(self, act_metrics):\n",
    "        self.act_metrics = act_metrics\n",
    "\n",
    "    def reformat_labels(self, labels):\n",
    "        # reformat labels to be delta xyz, delta rpy, next gripper state\n",
    "        new_labels = np.zeros_like(labels)\n",
    "        new_img_keys = []\n",
    "\n",
    "        for i in range(len(labels) - 1):\n",
    "            if i == 0:\n",
    "                current_label = labels[i]\n",
    "                next_label = labels[i + 1]\n",
    "            else:\n",
    "                next_label = labels[i + 1]\n",
    "\n",
    "            current_matrix = np.eye(4)\n",
    "            r = R.from_euler(\"xyz\", current_label[3:6], degrees=False)\n",
    "            current_matrix[:3, :3] = r.as_matrix()\n",
    "            current_matrix[:3, 3] = current_label[:3]\n",
    "\n",
    "            next_matrix = np.eye(4)\n",
    "            r = R.from_euler(\"xyz\", next_label[3:6], degrees=False)\n",
    "            next_matrix[:3, :3] = r.as_matrix()\n",
    "            next_matrix[:3, 3] = next_label[:3]\n",
    "\n",
    "            delta_matrix = np.linalg.inv(current_matrix) @ next_matrix\n",
    "            delta_xyz = delta_matrix[:3, 3]\n",
    "            delta_r = R.from_matrix(delta_matrix[:3, :3])\n",
    "            delta_rpy = delta_r.as_euler(\"xyz\", degrees=False)\n",
    "\n",
    "            del_gripper = next_label[6] - current_label[6]\n",
    "            xyz_norm = np.linalg.norm(delta_xyz)\n",
    "            rpy_norm = np.linalg.norm(delta_r.as_rotvec())\n",
    "\n",
    "            if xyz_norm < 0.01 and rpy_norm < 0.008 and abs(del_gripper) < 0.05:\n",
    "                # drop this label and corresponding image_key since the delta is too small (basically the same image)\n",
    "                continue\n",
    "\n",
    "            new_labels[i] = np.concatenate(\n",
    "                (delta_xyz, delta_rpy, np.array([next_label[6]]))\n",
    "            )\n",
    "            new_img_keys.append(self.img_keys[i])\n",
    "            current_label = next_label\n",
    "\n",
    "        # remove labels with all 0s\n",
    "        new_labels = new_labels[new_labels.sum(axis=1) != 0]\n",
    "        assert len(new_labels) == len(new_img_keys)\n",
    "        self.labels = new_labels\n",
    "        self.img_keys = new_img_keys\n",
    "\n",
    "    def load_labels(self, idx):\n",
    "        # load labels with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        labels = self.labels[\n",
    "            idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "        ]\n",
    "        # normalize labels\n",
    "        if self.act_metrics is not None:\n",
    "            labels = (labels - self.act_metrics[\"mean\"].numpy()) / self.act_metrics[\n",
    "                \"std\"\n",
    "            ].numpy()\n",
    "        return labels\n",
    "\n",
    "    def get_img_pths(self, idx):\n",
    "        # get image paths with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        img_keys = self.img_keys[\n",
    "            idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "        ]\n",
    "        img_pths = [self.img_pth / k for k in img_keys]\n",
    "        return img_pths\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.img_keys) - self.traj_len) // self.traj_skip + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError()\n",
    "        return None, self.load_labels(idx)\n",
    "\n",
    "\n",
    "class ImageStickDataset(StickDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        traj_path,\n",
    "        traj_len,\n",
    "        time_skip,\n",
    "        time_offset,\n",
    "        time_trim,\n",
    "        traj_skip,\n",
    "        img_size,\n",
    "        pre_load=False,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            traj_path, traj_len, time_skip, time_offset, time_trim, traj_skip\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.pre_load = pre_load\n",
    "        self.transforms = transforms\n",
    "        self.preprocess_img_transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(self.img_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        if self.pre_load:\n",
    "            self.imgs = self.load_imgs()\n",
    "\n",
    "    def load_imgs(self):\n",
    "        # load images in uint8 with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        imgs = []\n",
    "\n",
    "        for key in tqdm(self.img_keys):\n",
    "            img = Image.open(str(self.img_pth / key))\n",
    "            img = self.preprocess_img_transforms(img)\n",
    "            imgs.append(img)\n",
    "        # add a nex axis at the beginning\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "        return imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, labels = super().__getitem__(idx)\n",
    "\n",
    "        if self.pre_load:\n",
    "            imgs = self.imgs[\n",
    "                idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "            ]\n",
    "        else:\n",
    "            imgs = []\n",
    "            for key in self.img_keys[\n",
    "                idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "            ]:\n",
    "                img = Image.open(str(self.img_pth / key))\n",
    "                img = self.preprocess_img_transforms(img)\n",
    "                imgs.append(img)\n",
    "            # add a nex axis at the beginning\n",
    "            imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(imgs)\n",
    "\n",
    "        return imgs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837062e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_stick_dataset(\n",
    "    data_path,\n",
    "    traj_len=1,\n",
    "    traj_skip=1,\n",
    "    time_skip=4,\n",
    "    time_offset=5,\n",
    "    time_trim=5,\n",
    "    img_size=224,\n",
    "    pre_load=True,\n",
    "    apply_transforms=True,\n",
    "    val_mask=None,\n",
    "    mask_texts=None,\n",
    "    cfg=None,\n",
    "):\n",
    "    # add transforms for normalization and converting to float tensor\n",
    "    if type(data_path) == str:\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "    if apply_transforms:\n",
    "        transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transforms = None\n",
    "\n",
    "    train_traj_paths, val_traj_paths, test_traj_paths = iter_dir_for_traj_pths(\n",
    "        data_path, val_mask, mask_texts\n",
    "    )\n",
    "    # train_traj_paths = train_traj_paths[:64]\n",
    "    # val_traj_paths = val_traj_paths[:16]\n",
    "    # test_traj_paths = test_traj_paths[:16]\n",
    "    # concatenate all the Datasets for all the trajectories\n",
    "    train_dataset = ConcatDataset(\n",
    "        [\n",
    "            ImageStickDataset(\n",
    "                traj_path,\n",
    "                traj_len,\n",
    "                time_skip,\n",
    "                time_offset_n,\n",
    "                time_trim,\n",
    "                traj_skip,\n",
    "                img_size,\n",
    "                pre_load=pre_load,\n",
    "                transforms=transforms,\n",
    "            )\n",
    "            for traj_path, time_offset_n in itertools.product(\n",
    "                train_traj_paths, [time_offset, time_offset + 2]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    if len(val_traj_paths) > 0:\n",
    "        val_dataset = ConcatDataset(\n",
    "            [\n",
    "                ImageStickDataset(\n",
    "                    traj_path,\n",
    "                    traj_len,\n",
    "                    time_skip,\n",
    "                    time_offset,\n",
    "                    time_trim,\n",
    "                    traj_skip,\n",
    "                    img_size,\n",
    "                    pre_load=pre_load,\n",
    "                    transforms=transforms,\n",
    "                )\n",
    "                for traj_path in val_traj_paths\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        val_dataset = None\n",
    "\n",
    "    if len(test_traj_paths) > 0:\n",
    "        test_dataset = ConcatDataset(\n",
    "            [\n",
    "                ImageStickDataset(\n",
    "                    traj_path,\n",
    "                    traj_len,\n",
    "                    time_skip,\n",
    "                    time_offset,\n",
    "                    time_trim,\n",
    "                    traj_skip,\n",
    "                    img_size,\n",
    "                    pre_load=pre_load,\n",
    "                    transforms=transforms,\n",
    "                )\n",
    "                for traj_path in test_traj_paths\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903f20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ar7420/VINN/hello-robot-hack\n"
     ]
    }
   ],
   "source": [
    "%cd hello-robot-hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227b879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /scratch/ar7420/VINN/hello-robot-hack/.git/\n",
      "[master (root-commit) 25afa31] first commit\n",
      " 1 file changed, 1 insertion(+)\n",
      " create mode 100644 README.md\n",
      "Username for 'https://github.com': ^C\n"
     ]
    }
   ],
   "source": [
    "!echo \"# hello-robot-hack\" >> README.md\n",
    "!git init\n",
    "!git add README.md\n",
    "!git commit -m \"first commit\"\n",
    "!git branch -M main\n",
    "!git remote add origin https://github.com/RaiAnant/hello-robot-hack.git\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69f3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask = {'home': [],\n",
    "  'env': [],\n",
    "  'traj': ['2023-04-11--23-20-07_0', '2023-04-11--23-20-22_0']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c944f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through:  /vast/ar7420/iphone_data/Benchmarking_Export/Door_Opening/CDS_Home/Env1\n",
      "Total number of trajectories:  24\n",
      "total number of train trajectories:  24\n",
      "total number of test trajectories:  0\n",
      "Total number of val trajectories:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 129.30it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 140.59it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 141.46it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 139.20it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 141.21it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 140.90it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 141.31it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 141.96it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 139.92it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 142.03it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 139.94it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 138.54it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 141.37it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 139.09it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 138.86it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 139.37it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 136.52it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 143.70it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 137.19it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 144.49it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 143.59it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 145.19it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 141.27it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 145.03it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 138.56it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 139.29it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 141.77it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 143.64it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 143.81it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 137.77it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 137.61it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 137.84it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 138.58it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 145.03it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 145.40it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 145.28it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 142.63it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 145.75it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 146.17it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 141.97it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 145.92it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 146.06it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 142.92it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 113.97it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 100.52it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 108.74it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 112.72it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 115.00it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 107.84it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 103.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = get_image_stick_dataset(data_path='/vast/ar7420/iphone_data/Benchmarking_Export/Door_Opening/CDS_Home/Env1', img_size=[224, 224], val_mask = val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56419fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
