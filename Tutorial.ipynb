{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12addeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import abc\n",
    "import wandb\n",
    "from torchvision import transforms as T\n",
    "from byol_pytorch import BYOL\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# create abstract Dataset class called StickDataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "from utils.r3D_semantic_dataset import load_depth\n",
    "from utils.metrics import get_act_mean_std\n",
    "from utils.traverse_data import iter_dir_for_traj_pths\n",
    "\n",
    "\n",
    "class BaseStickDataset(Dataset, abc.ABC):\n",
    "    def __init__(self, traj_path, time_skip, time_offset, time_trim):\n",
    "        super().__init__()\n",
    "        self.traj_path = Path(traj_path)\n",
    "        self.time_skip = time_skip\n",
    "        self.time_offset = time_offset\n",
    "        self.time_trim = time_trim\n",
    "        self.img_pth = self.traj_path / \"images\"\n",
    "        self.depth_pth = self.traj_path / \"depths\"\n",
    "        self.conf_pth = self.traj_path / \"confs\"\n",
    "        self.labels_pth = self.traj_path / \"labels.json\"\n",
    "\n",
    "        self.labels = json.load(self.labels_pth.open(\"r\"))\n",
    "        self.img_keys = sorted(self.labels.keys())\n",
    "        # lable structure: {image_name: {'xyz' : [x,y,z], 'rpy' : [r, p, y], 'gripper': gripper}, ...}\n",
    "\n",
    "        self.labels = np.array(\n",
    "            [self.flatten_label(self.labels[k]) for k in self.img_keys]\n",
    "        )\n",
    "\n",
    "        # filter using time_skip and time_offset and time_trim. start from time_offset, skip time_skip, and remove last time_trim\n",
    "        self.labels = self.labels[: -self.time_trim][self.time_offset :: self.time_skip]\n",
    "\n",
    "        # filter keys using time_skip and time_offset and time_trim. start from time_offset, skip time_skip, and remove last time_trim\n",
    "        self.img_keys = self.img_keys[: -self.time_trim][\n",
    "            self.time_offset :: self.time_skip\n",
    "        ]\n",
    "\n",
    "    def flatten_label(self, label):\n",
    "        # flatten label\n",
    "        xyz = label[\"xyz\"]\n",
    "        rpy = label[\"rpy\"]\n",
    "        gripper = label[\"gripper\"]\n",
    "        return np.concatenate((xyz, rpy, np.array([gripper])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # not implemented\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class StickDataset(BaseStickDataset, abc.ABC):\n",
    "    def __init__(\n",
    "        self, traj_path, traj_len, time_skip, time_offset, time_trim, traj_skip\n",
    "    ):\n",
    "        super().__init__(traj_path, time_skip, time_offset, time_trim)\n",
    "        self.traj_len = traj_len\n",
    "        self.traj_skip = traj_skip\n",
    "        self.reformat_labels(self.labels)\n",
    "        self.act_metrics = None\n",
    "\n",
    "    def set_act_metrics(self, act_metrics):\n",
    "        self.act_metrics = act_metrics\n",
    "\n",
    "    def reformat_labels(self, labels):\n",
    "        # reformat labels to be delta xyz, delta rpy, next gripper state\n",
    "        new_labels = np.zeros_like(labels)\n",
    "        new_img_keys = []\n",
    "\n",
    "        for i in range(len(labels) - 1):\n",
    "            if i == 0:\n",
    "                current_label = labels[i]\n",
    "                next_label = labels[i + 1]\n",
    "            else:\n",
    "                next_label = labels[i + 1]\n",
    "\n",
    "            current_matrix = np.eye(4)\n",
    "            r = R.from_euler(\"xyz\", current_label[3:6], degrees=False)\n",
    "            current_matrix[:3, :3] = r.as_matrix()\n",
    "            current_matrix[:3, 3] = current_label[:3]\n",
    "\n",
    "            next_matrix = np.eye(4)\n",
    "            r = R.from_euler(\"xyz\", next_label[3:6], degrees=False)\n",
    "            next_matrix[:3, :3] = r.as_matrix()\n",
    "            next_matrix[:3, 3] = next_label[:3]\n",
    "\n",
    "            delta_matrix = np.linalg.inv(current_matrix) @ next_matrix\n",
    "            delta_xyz = delta_matrix[:3, 3]\n",
    "            delta_r = R.from_matrix(delta_matrix[:3, :3])\n",
    "            delta_rpy = delta_r.as_euler(\"xyz\", degrees=False)\n",
    "\n",
    "            del_gripper = next_label[6] - current_label[6]\n",
    "            xyz_norm = np.linalg.norm(delta_xyz)\n",
    "            rpy_norm = np.linalg.norm(delta_r.as_rotvec())\n",
    "\n",
    "            if xyz_norm < 0.01 and rpy_norm < 0.008 and abs(del_gripper) < 0.05:\n",
    "                # drop this label and corresponding image_key since the delta is too small (basically the same image)\n",
    "                continue\n",
    "\n",
    "            new_labels[i] = np.concatenate(\n",
    "                (delta_xyz, delta_rpy, np.array([next_label[6]]))\n",
    "            )\n",
    "            new_img_keys.append(self.img_keys[i])\n",
    "            current_label = next_label\n",
    "\n",
    "        # remove labels with all 0s\n",
    "        new_labels = new_labels[new_labels.sum(axis=1) != 0]\n",
    "        assert len(new_labels) == len(new_img_keys)\n",
    "        self.labels = new_labels\n",
    "        self.img_keys = new_img_keys\n",
    "\n",
    "    def load_labels(self, idx):\n",
    "        # load labels with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        labels = self.labels[\n",
    "            idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "        ]\n",
    "        # normalize labels\n",
    "        if self.act_metrics is not None:\n",
    "            labels = (labels - self.act_metrics[\"mean\"].numpy()) / self.act_metrics[\n",
    "                \"std\"\n",
    "            ].numpy()\n",
    "        return labels\n",
    "\n",
    "    def get_img_pths(self, idx):\n",
    "        # get image paths with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        img_keys = self.img_keys[\n",
    "            idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "        ]\n",
    "        img_pths = [self.img_pth / k for k in img_keys]\n",
    "        return img_pths\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.img_keys) - self.traj_len) // self.traj_skip + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError()\n",
    "        return None, self.load_labels(idx)\n",
    "\n",
    "\n",
    "class ImageStickDataset(StickDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        traj_path,\n",
    "        traj_len,\n",
    "        time_skip,\n",
    "        time_offset,\n",
    "        time_trim,\n",
    "        traj_skip,\n",
    "        img_size,\n",
    "        pre_load=False,\n",
    "        transforms=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            traj_path, traj_len, time_skip, time_offset, time_trim, traj_skip\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.pre_load = pre_load\n",
    "        self.transforms = transforms\n",
    "        self.preprocess_img_transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(self.img_size),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        if self.pre_load:\n",
    "            self.imgs = self.load_imgs()\n",
    "\n",
    "    def load_imgs(self):\n",
    "        # load images in uint8 with window size of traj_len, starting from idx and moving window by traj_skip\n",
    "        imgs = []\n",
    "\n",
    "        for key in tqdm(self.img_keys):\n",
    "            img = Image.open(str(self.img_pth / key))\n",
    "            img = self.preprocess_img_transforms(img)\n",
    "            imgs.append(img)\n",
    "        # add a nex axis at the beginning\n",
    "        imgs = torch.stack(imgs, dim=0)\n",
    "        return imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, labels = super().__getitem__(idx)\n",
    "\n",
    "        if self.pre_load:\n",
    "            imgs = self.imgs[\n",
    "                idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "            ]\n",
    "        else:\n",
    "            imgs = []\n",
    "            for key in self.img_keys[\n",
    "                idx * self.traj_skip : idx * self.traj_skip + self.traj_len\n",
    "            ]:\n",
    "                img = Image.open(str(self.img_pth / key))\n",
    "                img = self.preprocess_img_transforms(img)\n",
    "                imgs.append(img)\n",
    "            # add a nex axis at the beginning\n",
    "            imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(imgs)\n",
    "\n",
    "        return imgs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "837062e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_stick_dataset(\n",
    "    data_path,\n",
    "    traj_len=1,\n",
    "    traj_skip=1,\n",
    "    time_skip=4,\n",
    "    time_offset=5,\n",
    "    time_trim=5,\n",
    "    img_size=224,\n",
    "    pre_load=True,\n",
    "    apply_transforms=True,\n",
    "    val_mask=None,\n",
    "    mask_texts=None,\n",
    "    cfg=None,\n",
    "):\n",
    "    # add transforms for normalization and converting to float tensor\n",
    "    if type(data_path) == str:\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "    if apply_transforms:\n",
    "        transforms = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transforms = None\n",
    "\n",
    "    train_traj_paths, val_traj_paths, test_traj_paths = iter_dir_for_traj_pths(\n",
    "        data_path, val_mask, mask_texts\n",
    "    )\n",
    "    # train_traj_paths = train_traj_paths[:64]\n",
    "    # val_traj_paths = val_traj_paths[:16]\n",
    "    # test_traj_paths = test_traj_paths[:16]\n",
    "    # concatenate all the Datasets for all the trajectories\n",
    "    train_dataset = ConcatDataset(\n",
    "        [\n",
    "            ImageStickDataset(\n",
    "                traj_path,\n",
    "                traj_len,\n",
    "                time_skip,\n",
    "                time_offset_n,\n",
    "                time_trim,\n",
    "                traj_skip,\n",
    "                img_size,\n",
    "                pre_load=pre_load,\n",
    "                transforms=transforms,\n",
    "            )\n",
    "            for traj_path, time_offset_n in itertools.product(\n",
    "                train_traj_paths, [time_offset, time_offset + 2]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    if len(val_traj_paths) > 0:\n",
    "        val_dataset = ConcatDataset(\n",
    "            [\n",
    "                ImageStickDataset(\n",
    "                    traj_path,\n",
    "                    traj_len,\n",
    "                    time_skip,\n",
    "                    time_offset,\n",
    "                    time_trim,\n",
    "                    traj_skip,\n",
    "                    img_size,\n",
    "                    pre_load=pre_load,\n",
    "                    transforms=transforms,\n",
    "                )\n",
    "                for traj_path in val_traj_paths\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        val_dataset = None\n",
    "\n",
    "    if len(test_traj_paths) > 0:\n",
    "        test_dataset = ConcatDataset(\n",
    "            [\n",
    "                ImageStickDataset(\n",
    "                    traj_path,\n",
    "                    traj_len,\n",
    "                    time_skip,\n",
    "                    time_offset,\n",
    "                    time_trim,\n",
    "                    traj_skip,\n",
    "                    img_size,\n",
    "                    pre_load=pre_load,\n",
    "                    transforms=transforms,\n",
    "                )\n",
    "                for traj_path in test_traj_paths\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e69f3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask = {'home': [],\n",
    "  'env': [],\n",
    "  'traj': ['2023-04-11--23-20-07_0', '2023-04-11--23-20-22_0']}\n",
    "\n",
    "apply_transforms = False  ## We apply transforms in byol training so no need to pre apply transforms in the dataset\n",
    "img_size = [224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c944f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through:  data/Benchmarking_Export/Door_Opening/CDS_Home/Env1\n",
      "Total number of trajectories:  24\n",
      "total number of train trajectories:  24\n",
      "total number of test trajectories:  0\n",
      "Total number of val trajectories:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 113.61it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 140.98it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 143.82it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 147.41it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 147.71it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 152.05it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 149.91it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 133.97it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 147.45it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 141.24it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 148.01it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 145.62it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 151.06it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 153.08it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 147.71it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 151.85it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 136.80it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 152.42it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 148.75it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 153.46it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 150.52it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 149.07it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 146.52it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 145.40it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 124.26it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 116.65it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 132.58it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 141.52it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 143.89it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 133.94it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 134.51it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 121.40it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 129.54it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 132.81it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 134.12it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 133.01it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 138.10it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 141.85it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 111.33it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 139.66it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 124.74it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 143.57it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 148.00it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 148.61it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 148.72it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 150.21it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 130.57it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 148.22it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 149.99it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 106.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train, val, test = get_image_stick_dataset(data_path='data/Benchmarking_Export/Door_Opening/CDS_Home/Env1', img_size=img_size, val_mask = val_mask, apply_transforms = apply_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b15976",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an encoder using resnet18 or resnet50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    #Image encoder takes in an rgb image (or batch of images) as input and outputs N dimensional representation /encoding\n",
    "    def __init__(\n",
    "        self, encoder_type=\"resnet18\", pretrained=True, weights=None, cfg=None\n",
    "    ):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        if encoder_type == \"resnet18\":\n",
    "            #TODO: Complete the code block here\n",
    "        elif encoder_type == \"resnet50\":\n",
    "            #TODO: Complete the code block here\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported encoder type: {}\".format(encoder_type))\n",
    "\n",
    "        if weights:\n",
    "            self.encoder.load_state_dict(torch.load(weights))\n",
    "\n",
    "    def freeze(self):\n",
    "        #TODO: Complete the code block here\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f4434",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5887589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "#Base trainer class\n",
    "class Trainer(ABC):\n",
    "    def __init__(\n",
    "        self, model, optimizer, scheduler, device, experiment, save_every, cfg\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.experiment = experiment\n",
    "        self.save_every = save_every\n",
    "        self.cfg = cfg\n",
    "    \n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, train_dataloader, val_dataloader, test_dataloader, epochs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    def save(self, epoch):\n",
    "        if not os.path.exists(\"./weights\"):\n",
    "            os.makedirs(\"./weights\")\n",
    "        torch.save(self.model.state_dict(), f\"./weights/{self.experiment}_{epoch}.pth\")\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f0b51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOLTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self, model, optimizer, scheduler, device, experiment, save_every, img_size, cfg = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model, optimizer, scheduler, device, experiment, save_every, cfg\n",
    "        )\n",
    "        augmentation1 = ??\n",
    "        augmentation2 = ??\n",
    "        self.learner = BYOL(??)\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader, test_dataloader, epochs):\n",
    "\n",
    "        self.learner.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        if not os.path.exists(\"./weights\"):\n",
    "            os.makedirs(\"./weights\")\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            epoch_loss = 0\n",
    "            for i, (x, _) in enumerate(train_dataloader):\n",
    "                x = x.to(self.device)\n",
    "                x = x.squeeze()\n",
    "               #TODO: Complete the code block here\n",
    "            \n",
    "            \n",
    "               ######\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            wandb.log(\n",
    "                {\"loss\": epoch_loss / len(train_dataloader)}, step=epoch\n",
    "            )\n",
    "            if epoch % self.save_every == 0:\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    f\"./weights/{self.experiment}_{epoch}.pth\",\n",
    "                )\n",
    "\n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        pass\n",
    "\n",
    "    def test(self, dataloader):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894bba4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef18a1d",
   "metadata": {},
   "source": [
    "### Train Encoder using BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3c5edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"Tutorial_BYOL\"\n",
    "bs = 512\n",
    "lr = 3e-4\n",
    "epochs = 151\n",
    "num_workers = 4\n",
    "weight_decay = 1.5e-6\n",
    "save_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6b00c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train,\n",
    "    batch_size=bs,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val,\n",
    "    batch_size=bs,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08df0f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/py3.9/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b157e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "880c4273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gix9nqk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1381580... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fallen-jazz-2</strong>: <a href=\"https://wandb.ai/raianant/Tutorial_BYOL/runs/3gix9nqk\" target=\"_blank\">https://wandb.ai/raianant/Tutorial_BYOL/runs/3gix9nqk</a><br/>\n",
       "Find logs at: <code>./wandb/run-20230525_202731-3gix9nqk/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gix9nqk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.3 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/raianant/Tutorial_BYOL/runs/2rncfi1k\" target=\"_blank\">fanciful-music-3</a></strong> to <a href=\"https://wandb.ai/raianant/Tutorial_BYOL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project = experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7321bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "byol_trainer = BYOLTrainer(\n",
    "                model = model, \n",
    "                optimizer = optimizer, \n",
    "                scheduler = None, \n",
    "                device = 'cuda', \n",
    "                experiment = experiment, \n",
    "                save_every = save_every, \n",
    "                img_size = img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61786bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [06:00<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "byol_trainer.train(\n",
    "    train_loader, val_loader, None, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2329c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
